\SkipTocEntry\chapter{Imaging with a digital camera} 
\label{app:digital_imaging}
\appcaption{Appendix A}

With millions of pixels and the capacity to count tens of thousands of electrons
at each pixel, digital cameras are an engineering marvel. 
A basic understanding of how digital images are recorded serves to justify the
method of image normalization introduced in \autoref{ch:hvm}
and validates our model for generating synthetic
hologram images. This appendix briefly reviews the physical
principles of light detection by a single pixel to explain imaging effects such as
dark counts, saturation, and the digitization procedure. For a more detailed account,
consult reference \cite{nakamura2017}.

\section{Overview of digital imaging}

Digital cameras use an array of photon detectors to measure spatial variations in the
intensity of light filling a sensor's surface. Each photon detector, referred to as a pixel,
employs the photoelectric effect to convert the visible photons into
excited electrons.
The array of pixels is exposed to incoming light for a limited
exposure period during which excited electrons accumulate at each pixel.
The excited electrons are counted, sometimes to single precision,
and the counts are scaled into $8$-, $12$-, or even $16$-bit integers. As the number of
excited electrons is proportional to the number of photons, and the the number of
photons is in turn proportional to the intensity of the image at the
sensor surface, the array of digitized values counts serves as a proxy for
the intensity distribution of the image.

A small number of electrons can be thermally excited during the exposure period and
will be included in the count. This thermal background is referred to
as the dark count. The average dark count for a specific exposure period can be measured
by blocking the camera's sensor with an opaque object and averaging the
recorded images. The resulting averaged array is called a bias frame.
The dark count tends to increase with longer exposure times.
For an exposure period of less than \SI{10}{\us}, the dark
count of our camera is less than \SI{0.5}{\percent} the average intensity
at each pixel. We account for this additive offset by subtracting the bias
frame from each recorded frame.

Each pixel is designed and calibrated to closely approximate a linear relationship
between the number of photons incident on its surface and the represented pixel value.
This linear relationship fails, however, above a certain level of accumulated electrons,
$N_{\text{max}}$.
The registered number of electrons is therefore limited to a maximum value; a
pixel recording this maximum response is called a saturated pixel and is reporting
a lower bound for the intensity rather than an estimate of the intensity.
Unbiased measurements of intensity require that none of the pixels are saturated.
For ideal holographic imaging, the illumination should therefore be set such that the
brightest pixel in the image is below its saturation level.

The photons comprising the image arrive randomly at the camera
such that the average energy imparted by the photons is equal to the intensity
integrated over the sensor area and over the exposure period. Variation in the number
of arriving photons is referred to as shot noise.
The distribution describing the $n$ photons arriving during the exposure period
is modeled as a Poisson distribution,
\begin{align}
  P(n, \mu ) &= e^{-\mu} \frac{\mu^n}{n!}.
  \label{eq:poisson}
\end{align}
In the $\mu \gg 1$ limit, the Poisson distribution is well-approximated by a Gaussian distribution
with a mean of $\mu$, and a variance, $\sigma^2=\mu$. We
illuminate our sample with a \SI{447}{\nm} laser providing at most \SI{25}{\milli\watt}
through a beam diameter of \SI{3}{\milli\meter}. By imaging with a \SI{100}{\times} objective
lens and recording with a pixel size of \SI{13.5}{\um}, we expect $\mu \approx \num{10000}$
photons during the \SI{10}{\us} exposure period. We therefore reasonably model
fluctuations in the observed pixel value as a Gaussian distribution.

Each pixel of our $8$-bit camera can accommodate approximately \num{30000} electrons before
it saturates. In addition, the first
$32$ values are reserved to account for negative currents and maintain Gaussian
error in intensity measured. Therefore the units of observed intensity reported by the
camera correspond to $30000/(256-32) \approx 134$ excited electrons. This error
caused by flooring the electron readout is known as quantization noise and, in our case,
is commensurate to the shot noise.

Background measurements reveal that the illumination is not perfectly uniform. The illumination
deviates as much as \SI{5}{\percent} across the image and is presumably accounted for by the
long coherence length of the laser imaging dust on and imperfections in the optics.
The spatial heterogeneity imparted by the structure in the illumination is almost
entirely accounted for by proper normalization \cite{lee07a}. When fitting holographic
features or synthesizing holograms, we account for shot noise,
quantization noise, and background as collectively contributing a Gaussian error
of \SI{5}{\percent} the average intensity illuminating the sample.



%During an exposure period, $N_p$ photons of a particular wavelength arrive at a pixel
%surface. Some fraction of the incident photons are converted to excited electrons
%with a wavelength dependent probability known as the quantum efficiency. To be properly
%counted, these excited electrons must survive until the counting procedure has
%accounted for their presence. To this end, each pixel is doped to increase the lifetime
%of excited electrons. In addition, the excited electrons must remain in the bulk so that
%they are not grounded; for this purpose, a biased field is applied.

% The number of electrons that can be negative.. scientific cameras have a non-zero
% floor to maintain gaussian-errors.
% Saturation occurs because the relation number of excited electrons per
% number of incident photons becomes non-linear. The largest number of reported
% electrons is the highest level

% A number of approximations come to mind with the LM theory.
% Approximation of radial component
% Approximation of functional form (Hankel function)
% Approximation of polarization

% What are digital images. How are digital images recorded?
% Why are you telling them such things?
% How is it that digital images do not record intensity?
